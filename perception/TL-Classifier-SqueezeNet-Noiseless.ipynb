{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Light Classifier\n",
    "**In this notebook, a deep learning approach is used for the purposes of detecting traffic lights and their corresponding state representation (i.e. Red, Green, Yellow).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Define the training and test paths...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import model.modelconfig as config\n",
    "\n",
    "# Define training data paths\n",
    "training_path = \"./data/train/images\"\n",
    "test_path = \"./data/test/images\"\n",
    "\n",
    "model_path = \"./model/squeeze-noiseless/\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 38868\n",
      "Number of test samples: 23135\n"
     ]
    }
   ],
   "source": [
    "# Extract number of samples\n",
    "\n",
    "def get_samples(path, patterns = (\"*.png\", \"*.jpg\")):\n",
    "    paths = []\n",
    "    for key in list(config.labels.keys()):\n",
    "        dir = os.path.join(path, key)\n",
    "        for pattern in patterns:\n",
    "            paths += glob.glob(os.path.join(dir, pattern))\n",
    "    return paths\n",
    "\n",
    "train_samples = get_samples(training_path)\n",
    "test_samples = get_samples(test_path)\n",
    "\n",
    "print(\"Number of training samples: {}\".format(len(train_samples)))\n",
    "print(\"Number of test samples: {}\".format(len(test_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to disable GPU acceleration\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.0.8\n",
      "TensorFlow version: 1.0.0\n",
      "Channels: channels_last\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import keras;\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "# Check keras and tensorflow versions\n",
    "print(\"Keras version: {}\".format(keras.__version__))\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Channels: {}\".format(K.image_data_format()))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found. Using CPU...')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SqueezeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: \n",
      "[0, 1, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "class_labels = list(set(config.labels.values()))\n",
    "print(\"Class labels: \\n{}\".format(class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model params\n",
    "output_size = len(class_labels)\n",
    "image_size = (None, None, 3)\n",
    "\n",
    "dropout = 0.0\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Build model\n",
    "from model.SqueezeNet import SqueezeNet\n",
    "\n",
    "model = SqueezeNet(output_size, image_size, dropout, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Optimiser for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Model ##\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "normalizer_1 (Normalizer)        (None, None, None, 3) 0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, None, None, 96 14208       normalizer_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)          (None, None, None, 96 0           conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fire2_squeeze (Conv2D)           (None, None, None, 16 1552        maxpool1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fire2_expand1 (Conv2D)           (None, None, None, 64 1088        fire2_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire2_expand2 (Conv2D)           (None, None, None, 64 9280        fire2_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, None, None, 12 0           fire2_expand1[0][0]              \n",
      "                                                                   fire2_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire3_squeeze (Conv2D)           (None, None, None, 16 2064        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire3_expand1 (Conv2D)           (None, None, None, 64 1088        fire3_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire3_expand2 (Conv2D)           (None, None, None, 64 9280        fire3_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, None, None, 12 0           fire3_expand1[0][0]              \n",
      "                                                                   fire3_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire4_squeeze (Conv2D)           (None, None, None, 32 4128        concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire4_expand1 (Conv2D)           (None, None, None, 12 4224        fire4_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire4_expand2 (Conv2D)           (None, None, None, 12 36992       fire4_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, None, None, 25 0           fire4_expand1[0][0]              \n",
      "                                                                   fire4_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpool4 (MaxPooling2D)          (None, None, None, 25 0           concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire5_squeeze (Conv2D)           (None, None, None, 32 8224        maxpool4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fire5_expand1 (Conv2D)           (None, None, None, 12 4224        fire5_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire5_expand2 (Conv2D)           (None, None, None, 12 36992       fire5_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, None, None, 25 0           fire5_expand1[0][0]              \n",
      "                                                                   fire5_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire6_squeeze (Conv2D)           (None, None, None, 48 12336       concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire6_expand1 (Conv2D)           (None, None, None, 19 9408        fire6_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire6_expand2 (Conv2D)           (None, None, None, 19 83136       fire6_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, None, None, 38 0           fire6_expand1[0][0]              \n",
      "                                                                   fire6_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire7_squeeze (Conv2D)           (None, None, None, 48 18480       concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire7_expand1 (Conv2D)           (None, None, None, 19 9408        fire7_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire7_expand2 (Conv2D)           (None, None, None, 19 83136       fire7_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, None, None, 38 0           fire7_expand1[0][0]              \n",
      "                                                                   fire7_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire8_squeeze (Conv2D)           (None, None, None, 64 24640       concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire8_expand1 (Conv2D)           (None, None, None, 25 16640       fire8_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire8_expand2 (Conv2D)           (None, None, None, 25 147712      fire8_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, None, None, 51 0           fire8_expand1[0][0]              \n",
      "                                                                   fire8_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpool8 (MaxPooling2D)          (None, None, None, 51 0           concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire9_squeeze (Conv2D)           (None, None, None, 64 32832       maxpool8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fire9_expand1 (Conv2D)           (None, None, None, 25 16640       fire9_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire9_expand2 (Conv2D)           (None, None, None, 25 147712      fire9_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, None, None, 51 0           fire9_expand1[0][0]              \n",
      "                                                                   fire9_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                  (None, None, None, 4) 2052        concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "avgpool10 (GlobalAveragePooling2 (None, 4)             0           conv10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 4)             0           avgpool10[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 737,476\n",
      "Trainable params: 737,476\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Optimiser params\n",
    "optimiser = \"Adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"categorical_accuracy\", \"top_k_categorical_accuracy\"]\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimiser, loss, metrics=metrics, loss_weights=None, sample_weight_mode=None)\n",
    "\n",
    "print(\"\\n## Model ##\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch (train): 3239\n",
      "Steps per epoch (test): 1927\n"
     ]
    }
   ],
   "source": [
    "# Train on Bosch dataset\n",
    "### Training parameters ###\n",
    "noise = 0.0\n",
    "batch_size = 12\n",
    "max_epochs = 40\n",
    "\n",
    "epoch_train_samples = int(floor(len(train_samples) / batch_size))\n",
    "epoch_test_samples = int(floor(len(test_samples) / batch_size))\n",
    "\n",
    "print(\"Steps per epoch (train): {}\".format(epoch_train_samples))\n",
    "print(\"Steps per epoch (test): {}\".format(epoch_test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Define image data generators and create noise on training set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing generators...\n",
      " - Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import data.helpers.generator as g\n",
    "\n",
    "print(\"\\nInitializing generators...\")\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = g.generator(training_path, config.labels, batch_size = batch_size, noise = noise)\n",
    "validation_generator = g.generator(test_path, config.labels, batch_size = batch_size, noise = 0)\n",
    "\n",
    "print(\" - Done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Define callbacks for use during training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train ###\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# setup callbacks, save every checkpoint, check for overfitting and reduce learning rate as required\n",
    "callbacks = [\n",
    "    CSVLogger(model_path + \"/training-loss.csv\", separator=',', append = True),\n",
    "    #EarlyStopping(monitor='val_loss', min_delta = 0.0001, patience = 7),\n",
    "    ModelCheckpoint(filepath = model_path + config.checkpoint_pattern.replace(\"*\",\"{epoch:02d}-{val_loss:.2f}\"), monitor='val_loss', save_best_only=False, save_weights_only=True, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "Epoch 1/40\n",
      "3239/3239 [==============================] - 163s - loss: 0.9325 - categorical_accuracy: 0.6322 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.9907 - val_categorical_accuracy: 0.6347 - val_top_k_categorical_accuracy: 0.9995\n",
      "Epoch 2/40\n",
      "3239/3239 [==============================] - 142s - loss: 0.9283 - categorical_accuracy: 0.6324 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.9889 - val_categorical_accuracy: 0.6347 - val_top_k_categorical_accuracy: 0.9995\n",
      "Epoch 3/40\n",
      "3239/3239 [==============================] - 141s - loss: 0.9278 - categorical_accuracy: 0.6324 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.0037 - val_categorical_accuracy: 0.6343 - val_top_k_categorical_accuracy: 0.9995\n",
      "Epoch 4/40\n",
      "1672/3239 [==============>...............] - ETA: 58s - loss: 0.9338 - categorical_accuracy: 0.6285 - top_k_categorical_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-afdf91c133d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m history = model.fit_generator(train_generator, steps_per_epoch = epoch_train_samples, \n\u001b[0;32m      6\u001b[0m                               \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_test_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                               epochs = max_epochs, callbacks = callbacks, initial_epoch = epoch)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining...\")\n",
    "\n",
    "epoch = 0\n",
    "# Train the model using the supplied generator\n",
    "history = model.fit_generator(train_generator, steps_per_epoch = epoch_train_samples, \n",
    "                              validation_data = validation_generator, validation_steps = epoch_test_samples,\n",
    "                              epochs = max_epochs, callbacks = callbacks, initial_epoch = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_path = \"./data/valid/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "labelnames = []\n",
    "\n",
    "for dir in os.listdir(valid_path):\n",
    "    sub_path = os.path.join(valid_path, dir)\n",
    "    if (os.path.isdir(sub_path) and dir in config.labels):\n",
    "        for img_name in os.listdir(sub_path):\n",
    "            img_path = os.path.join(sub_path, img_name)\n",
    "            images.append(img_path)\n",
    "            labels.append(config.labels[dir])\n",
    "            labelnames.append(dir)\n",
    "\n",
    "num_rows = 6\n",
    "num_cols = 4\n",
    "\n",
    "fig = plt.figure(figsize=(18, 16))  # width, height in inches\n",
    "\n",
    "k = 1\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    image = mpimg.imread(images[i])\n",
    "    label = labels[i]\n",
    "    name = labelnames[i]\n",
    "    \n",
    "    image = np.asarray(image)\n",
    "    \n",
    "    pred = model.predict(image[None, :, :, 0:3], batch_size=1)[0]\n",
    "    \n",
    "    pred_idx = np.argmax(pred)\n",
    "    pred_label = list(config.labels)[pred_idx]\n",
    "    pred_score = np.round(pred[pred_idx], 6)\n",
    "    \n",
    "    sub = fig.add_subplot(num_rows, num_cols, k)\n",
    "    sub.set_title(\"{} / {}\\n{}\".format(name, pred_label, str(pred_score)), fontsize=9)\n",
    "    sub.imshow(image)\n",
    "    #sub.axis(\"off\")\n",
    "    \n",
    "    color = \"red\"\n",
    "    if (pred_label == label):\n",
    "        color = \"lime\"\n",
    "            \n",
    "    #sub.set_label(str(pred_score))#, fontsize=8, color=color)\n",
    "    sub.axis('off')\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "fig.suptitle(\"Traffic Lights\",fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write out model params to file for use in project"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
